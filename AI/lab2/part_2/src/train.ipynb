{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class char_tokenizer:\n",
    "    \"\"\"\n",
    "    a very simple char-based tokenizer. the tokenizer turns a string into a list of integers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus: List[str]):\n",
    "        self.corpus = corpus\n",
    "        # TODO: calculate the vocab size and create a dictionary that maps each character to a unique integer\n",
    "        self.vocab_size = len(corpus)\n",
    "        self.char2int = {char: i for i, char in enumerate(corpus)}\n",
    "\n",
    "    def encode(self, string: str):\n",
    "        # TODO: convert a string into a list of integers and return, using the dictionary you created above\n",
    "        return [self.char2int[char] for char in string]\n",
    "\n",
    "    def decode(self, codes: List[int]):\n",
    "        # TODO: convert a list of integers into a string and return, using the dictionary you created above\n",
    "        return ''.join([self.corpus[code] for code in codes])\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"single head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # TODO: create three linear layers, Key, Query, and Value, each of which maps from n_embd to head_size\n",
    "        self.Key = nn.Linear(head_size, head_size)\n",
    "        self.Query = nn.Linear(head_size, head_size)\n",
    "        self.Value = nn.Linear(head_size, head_size)\n",
    "\n",
    "        self.register_buffer(\"tril\", torch.tril(\n",
    "            torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # TODO: implement the forward function of the head\n",
    "        # the input is a tensor of shape (batch, time, n_embd)\n",
    "        # the output should be a tensor of shape (batch, time, head_size)\n",
    "        # you may use the tril buffer defined above to mask out the upper triangular part of the affinity matrix\n",
    "\n",
    "        batch_size, time, n_embd = inputs.size()\n",
    "\n",
    "        # Compute keys, queries, and values\n",
    "        keys = self.Key(inputs)\n",
    "        queries = self.Query(inputs)\n",
    "        values = self.Value(inputs)\n",
    "\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(queries, keys.transpose(1, 2))\n",
    "        scores = scores.masked_fill(\n",
    "            self.tril[:time, :time] == 0, float('-inf'))\n",
    "\n",
    "        # Normalize attention scores\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        out = torch.matmul(attention_weights, values)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        # TODO: implement heads and projection\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.projection = nn.Linear(n_heads * head_size, head_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # TODO: implement the forward function of the multi-head attention\n",
    "        head_outputs = [head(inputs) for head in self.heads]\n",
    "        out = torch.cat(head_outputs, dim=-1)\n",
    "        return self.projection(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        # TODO: implement the feed-forward network\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.net(inputs)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads):\n",
    "        super().__init__()\n",
    "        # TODO: implement the block of transformer using the MultiHeadAttention and FeedForward modules,\n",
    "        # along with the layer normalization layers\n",
    "        self.attention = MultiHeadAttention(n_heads, n_embd)\n",
    "        self.norm1 = nn.LayerNorm(n_embd)\n",
    "        self.ffn = FeedForward(n_embd)\n",
    "        self.norm2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # TODO: implement the forward function of the block, you may refer to the docs of this experiment\n",
    "        attention_output = self.attention(inputs)\n",
    "        attention_output = self.norm1(inputs + attention_output)\n",
    "\n",
    "        ffn_output = self.ffn(attention_output)\n",
    "        out = self.norm2(attention_output + ffn_output)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: create the embedding table, the stack of blocks, the layer normalization layer,\n",
    "        # and the linear layers.\n",
    "        self.embedding = nn.Embedding(tokenizer.vocab_size, n_embd)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(n_embd, n_heads) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(n_embd)\n",
    "        self.linear = nn.Linear(n_embd, tokenizer.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        # TODO: implement the forward function of the transformer\n",
    "\n",
    "        # inputs:(batch, context)\n",
    "        batch, context = inputs.shape\n",
    "\n",
    "        # embedding:(batch, context, embedding)\n",
    "        embedding_output = self.embedding(inputs)\n",
    "\n",
    "        # attens:(batch, context, embedding)\n",
    "        attens = embedding_output\n",
    "\n",
    "        for block in self.blocks:\n",
    "            attens = block(attens)\n",
    "\n",
    "        # logits:(batch, context, attens)\n",
    "        logits = self.linear(self.norm(attens))\n",
    "\n",
    "        # compute the loss\n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        # TODO: generate new tokens from the transformer, using the inputs as the context,\n",
    "        # and return the generated tokens with length of max_new_tokens\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            output, _ = self(inputs[:, -block_size + 1:])\n",
    "            predicted_token = output.argmax(dim=-1)[:, -1]\n",
    "            inputs = torch.cat([inputs, predicted_token[:, None]], dim=-1)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    return out\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "\n",
    "        if iter % eval_interval == 0:\n",
    "            losses = estimate_loss(model)\n",
    "            print(\n",
    "                f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
    "            )\n",
    "\n",
    "        inputs, labels = get_batch(\"train\")\n",
    "\n",
    "        logits, loss = model(inputs, labels)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# define the hyperparameters\n",
    "batch_size = 16\n",
    "block_size = 256\n",
    "max_iters = 5000  # set the number of training iterations as you like\n",
    "eval_interval = 50\n",
    "learning_rate = 1e-3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_heads = 8\n",
    "n_layers = 8\n",
    "\n",
    "# read the dataset\n",
    "with open(\"../data/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# initialize the vocabulary\n",
    "tokenizer = char_tokenizer(chars)\n",
    "encode = tokenizer.encode\n",
    "decode = tokenizer.decode\n",
    "n_vocab = tokenizer.vocab_size\n",
    "\n",
    "# separate the dataset into train and validation\n",
    "train_data = torch.tensor(encode(text[: -len(text) // 10]), device=device, dtype=torch.long)\n",
    "val_data = torch.tensor(encode(text[-len(text) // 10:]), device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2906, val loss 4.2902\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Transformer().to(device)\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './save.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer()\n",
    "model.to(device=device)\n",
    "model.load_state_dict(torch.load('./save.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "I am to the sea that were the sea to the sease that were the sease\n",
      "That the seasure of the seasure of the sease the seast\n",
      "That with anting struck and the seasurled of the seast,\n",
      "And there is the seasure of the seasure of the seast,\n",
      "And there is the seasurlesss of the seast the seast,\n",
      "And there is the seasurless of the seasure of the seasure\n",
      "That the seasure of the seasure of the seast, and though art\n",
      "That he with antest of the seasurless of the seast,\n",
      "And there is the seasurless of the seasurit\n",
      "That with a soul happiness of the seasurled of the seast,\n",
      "And there is the seasurless of the seasurit\n",
      "As the seasure of the seasure of the seasure of the seast\n",
      "That with the seasuring of the seasure of the seasure\n",
      "I shall be the seasure of the seasure of the seast,\n",
      "And there is the seasurless of the seasurit\n",
      "That with an the seasuring of the seasure of the seast,\n",
      "And there is the seasuring of the seasure of the seas,\n",
      "And there is the seasure of the seasure of the seasure\n",
      "That with the seasure o\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate(model):\n",
    "    start_text = \"All:\"\n",
    "    start_tokens = tokenizer.encode(start_text)\n",
    "    context = torch.tensor([start_tokens], device=device, dtype=torch.long)\n",
    "    # context = torch.zeros((1, 1), device=device, dtype=torch.long)\n",
    "    print(decode(model.generate(context, max_new_tokens=1000)[0].tolist()))\n",
    "\n",
    "generate(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
